# -*- coding: utf-8 -*-
"""Sistem Rekomendasi Ekowisata di Indonesia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JhQ0a-rJiBWAYemgfxAoQ_R_BE_9aVBM

# **Sistem Rekomendasi Ekowisata di Indonesia â€“ Imelda Cyntia**

## Ringkasan

Proyek ini bertujuan untuk membangun sistem rekomendasi ekowisata di Indonesia yang mampu memberikan saran destinasi secara personal dan relevan kepada pengguna. Sistem dikembangkan dengan menggunakan dua pendekatan, yaitu Content-Based Filtering dan Collaborative Filtering.

Dengan adanya sistem ini, diharapkan promosi destinasi wisata berbasis alam dan budaya lokal dapat lebih tersebar merata dan tepat sasaran, khususnya ke tempat-tempat yang selama ini kurang terekspos di platform digital.

## Sumber Dataset

Dataset yang digunakan berasal dari Kaggle:
 [Dataset *Indonesiaâ€™s Ecotourism* â€“ Kaggle](https://www.kaggle.com/datasets/rizkysetiawann/indonesias-ecotourism-dataset)

Dataset terdiri dari tiga bagian utama, yaitu:

1. eco\_place.csv
   Berisi data detail mengenai tempat ekowisata, termasuk nama tempat, deskripsi, kategori wisata, lokasi (kota dan provinsi), harga tiket, rating, serta koordinat geografis.

2. eco\_event.csv
   Menyediakan informasi mengenai acara atau kegiatan yang pernah diselenggarakan di destinasi tersebut. Data ini berguna untuk memperkaya rekomendasi berdasarkan aktivitas yang mungkin diminati pengguna.

3. eco\_rating.csv
   Berisi data penilaian atau rating yang diberikan pengguna terhadap tempat wisata tertentu. Data ini menjadi dasar untuk penerapan Collaborative Filtering dalam sistem rekomendasi.

# **Import Libraries**
"""

# Import library utama untuk analisis data dan visualisasi
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Import library untuk Content-based Filtering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Import library TensorFlow untuk deep learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers

# Import untuk preprocessing
from sklearn.preprocessing import LabelEncoder

# Import Path untuk memudahkan pengelolaan path file
from pathlib import Path

# Import untuk menampilkan tabel dengan format rapi
from tabulate import tabulate

"""# **Data Understanding**"""

# Upload file CSV
from google.colab import files
uploaded = files.upload()

# Mengubah nama file dataset agar lebih ringkas
!mv '/content/eco_place.csv' 'places.csv'
!mv '/content/eco_event.csv' 'events.csv'
!mv '/content/eco_rating.csv' 'ratings.csv'

# Load ulang dengan nama file baru
df_place = pd.read_csv('places.csv')
df_event = pd.read_csv('events.csv')
df_rating = pd.read_csv('ratings.csv')

"""#### Dataset Destinasi Wisata"""

# Menampilkan informasi struktur dari dataset destinasi wisata
df_place.info()

# Menampilkan 5 baris pertama dari dataset tempat wisata
df_place.head()

# Cek missing value
print(df_place.isnull().sum())

"""Insight singkat dari dataset tempat wisata
* Total baris data: 182
* Total kolom fitur: 13
* Terdapat 2 kolom bertipe numerik: place_id, rating
* Terdapat 11 kolom bertipe object (string), termasuk nama tempat, kategori, harga, deskripsi, dan link gambar
* Ditemukan missing value pada:
 * gallery_photo_img2: 2 data kosong
 * gallery_photo_img3: 77 data kosong

#### Dataset Event Wisata
"""

# Menampilkan informasi struktur dari dataset event wisata
df_event.info()

# Menampilkan 5 baris pertama dari dataset event wisata
df_event.head()

# Cek missing value
print(df_event.isnull().sum())

"""Insight singkat dari dataset event wisata

* Total baris data: 6
* Total kolom fitur: 6
* Terdapat 1 kolom bertipe numerik: `event_id`
* Terdapat 5 kolom bertipe object: `event_img`, `event_name`, `event_place`, `event_date`, `event_about`
* Tidak ditemukan missing value di seluruh kolom

#### Dataset Rating Pengguna
"""

# Menampilkan informasi struktur dari dataset rating oleh pengguna
df_rating.info()

# Menampilkan 5 baris pertama dari dataset rating pengguna
df_rating.head()

# Cek missing value
print(df_rating.isnull().sum())

"""Insight singkat dari dataset rating

* Total baris data: 849
* Total kolom fitur: 3
* Seluruh kolom (user_id, place_id, user_rating) bertipe numerik (int64)
* Tidak ditemukan missing value

# **Exploratory Data Analysis**

##### 1. Place:
Berisi informasi detail tentang tempat-tempat wisata berbasis ekowisata di Indonesia.

* `place_id`: ID unik untuk setiap destinasi wisata.
* `place_name`: Nama dari destinasi wisata.
* `place_description`: Deskripsi mengenai tempat wisata.
* `category`: Kategori wisata (misal: alam, sejarah, edukasi, budaya, dll).
* `city`: Lokasi kota/kabupaten tempat wisata berada.
* `price`: Harga tiket masuk (biasanya dalam format string, perlu preprocessing).
* `rating`: Nilai rata-rata penilaian pengguna (skala 1â€“5).
* `description_location`: Deskripsi lokasi tempat wisata.
* `place_img`: URL gambar utama dari destinasi.
* `gallery_photo_img1`: URL foto galeri 1.
* `gallery_photo_img2`: URL foto galeri 2 (memiliki 2 missing value).
* `gallery_photo_img3`: URL foto galeri 3 (memiliki 77 missing value).
* `place_map`: URL peta lokasi dari destinasi wisata.

##### 2. Event:
Berisi event atau acara yang berkaitan dengan destinasi ekowisata.

* `event_id`: ID unik untuk setiap event.
* `event_img`: URL gambar promosi event.
* `event_name`: Nama event yang diselenggarakan.
* `event_place`: Lokasi destinasi tempat event berlangsung.
* `event_date`: Tanggal pelaksanaan event.
* `event_about`: Deskripsi atau rincian tentang event.

##### 3. Rating:

Berisi penilaian pengguna terhadap destinasi wisata tertentu.

* `user_id`: ID unik pengguna.
* `place_id`: ID destinasi wisata yang dinilai (mengacu ke `eco_place.csv`).
* `user_rating`: Rating yang diberikan pengguna terhadap destinasi (skala 1â€“5).

####  1. Eksplorasi Awal Tempat Wisata
"""

# Menampilkan 1 baris pertama dari tempat wisata
df_place.head(1)

# Menampilkan jumlah tempat wisata unik berdasarkan kolom 'place_name'
jumlah_tempat_unik = df_place['place_name'].nunique()
print(f"Jumlah Tempat Wisata Unik: {jumlah_tempat_unik}")

# Membuat tabel berisi pasangan unik place_id dan place_name
tabel_tempat_wisata = (
    df_place[['place_id', 'place_name']]
    .drop_duplicates()
    .sort_values(by='place_id')
    .reset_index(drop=True)
)

# Tampilkan data dalam format tabel tanpa index
print(tabel_tempat_wisata.to_string(index=False))

"""Dataset ini mencakup 182 tempat wisata unik di Indonesia, yang tersebar di berbagai kategori ekowisata seperti taman nasional, desa wisata, pantai, air terjun, dan situs budaya. Keanekaragaman destinasi ini mencerminkan potensi besar Indonesia dalam pengembangan ekowisata berbasis alam dan budaya lokal."""

# Ambil 10 data pertama dari tabel_tempat_wisata
top_10_tempat = tabel_tempat_wisata.head(10)

# Visualisasi bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_10_tempat['place_name'], top_10_tempat['place_id'])
plt.xlabel('Place ID')
plt.title('10 Tempat Wisata Pertama Berdasarkan Place ID')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""Visualisasi ini menampilkan 10 tempat wisata pertama berdasarkan place_id. Urutannya konsisten dengan ID terkecil hingga terbesar, yang mencerminkan urutan input data bukan popularitas atau frekuensi. Ini berguna untuk melihat daftar awal tempat wisata unik dalam dataset.

#### 2. Distribusi Kategori Destinasi Wisata
"""

# Pisahkan kategori yang digabung (menggunakan koma) menjadi kategori individual
kategori_terpisah = df_place['category'].str.split(',', expand=True).stack().reset_index(drop=True)
kategori_terpisah.name = 'kategori_individual'

# Hitung distribusi kategori individual
frekuensi_kategori = kategori_terpisah.value_counts()

# Tampilkan jumlah kategori unik dan distribusinya
print(f"Jumlah Kategori Individual: {frekuensi_kategori.shape[0]}")
print("\nDistribusi Jumlah Destinasi per Kategori (Terpisah):")
print(frekuensi_kategori)

# Visualisasi kategori individual
plt.figure(figsize=(6, 4))
sns.barplot(x=frekuensi_kategori.values, y=frekuensi_kategori.index)
plt.title('Distribusi Kategori Destinasi Wisata (Terpisah)')
plt.xlabel('Jumlah Destinasi')
plt.ylabel('Kategori')
plt.tight_layout()
plt.show()

"""Kategori Cagar Alam merupakan yang paling dominan dalam dataset ini dengan 146 destinasi, disusul oleh Budaya, Bahari, dan Taman Nasional. Hal ini menunjukkan bahwa fokus utama ekowisata di Indonesia masih terpusat pada pelestarian alam dan budaya lokal.

#### 3. Sebaran Destinasi Berdasarkan Kota
"""

# Menampilkan jumlah kota unik dalam dataset
jumlah_kota = df_place['city'].nunique()
print(f"Total Kota/Kabupaten Tercatat: {jumlah_kota}")

# Hitung jumlah destinasi untuk setiap kota
jumlah_destinasi_per_kota = df_place['city'].value_counts().sort_values(ascending=False)

# Tampilkan daftar kota beserta jumlah destinasi
print("\nJumlah Destinasi per Kota/Kabupaten:")
print(jumlah_destinasi_per_kota)

# Ambil 10 kota teratas berdasarkan jumlah destinasi
top_kota = jumlah_destinasi_per_kota.head(10)

# Visualisasi menggunakan bar chart horizontal
plt.figure(figsize=(8, 5))
sns.barplot(x=top_kota.values, y=top_kota.index)
plt.title('10 Kota/Kabupaten dengan Destinasi Terbanyak', fontsize=12)
plt.xlabel('Jumlah Destinasi')
plt.ylabel('Kota/Kabupaten')
plt.tight_layout()
plt.show()

"""Kota Yogyakarta dan Bandung menempati posisi teratas dengan jumlah destinasi wisata terbanyak, menunjukkan potensi wisata yang sangat kuat di kedua daerah tersebut. Dominasi kota-kota di Pulau Jawa dalam daftar ini mengindikasikan konsentrasi pengembangan dan dokumentasi wisata yang lebih tinggi di wilayah tersebut.

#### 4. Interaksi Pengguna per Destinasi
"""

# Gabungkan dengan nama tempat
top_rating_place = df_rating.groupby('place_id').size().reset_index(name='count')
top_rating_place = top_rating_place.sort_values(by='count', ascending=False).head(10)

# Gabungkan dengan nama destinasi
top_rating_place = top_rating_place.merge(df_place[['place_id', 'place_name']], on='place_id')

# Visualisasi
plt.figure(figsize=(8, 5))
sns.barplot(x='count', y='place_name', data=top_rating_place)
plt.title('10 Destinasi dengan Jumlah Rating Terbanyak')
plt.xlabel('Jumlah Rating')
plt.ylabel('Nama Tempat')
plt.tight_layout()
plt.show()

"""Destinasi seperti Dusun Bambu, Curug Cilengkrang, dan Goa Rancang Kencono termasuk yang paling sering dirating, menandakan popularitas atau keterlibatan pengguna yang tinggi terhadap destinasi ini.

#### 5. Rata-rata Rating pada Destinasi Terpopuler
"""

# Rata-rata rating per destinasi (hanya untuk top 10 place_id di atas)
avg_rating_top = df_rating[df_rating['place_id'].isin(top_rating_place['place_id'])]
avg_rating_top = avg_rating_top.groupby('place_id')['user_rating'].mean().reset_index(name='avg_rating')

# Gabungkan rata-rata rating dengan nama tempat
avg_rating_top = avg_rating_top.merge(df_place[['place_id', 'place_name']], on='place_id')

# Urutkan berdasarkan rata-rata rating
avg_rating_top = avg_rating_top.sort_values(by='avg_rating', ascending=False)

# Visualisasi rata-rata rating (quality)
plt.figure(figsize=(8, 5))
sns.barplot(x='avg_rating', y='place_name', data=avg_rating_top)
plt.title('Rata-rata Rating pada 10 Destinasi Terpopuler')
plt.xlabel('Rata-rata Rating')
plt.ylabel('Nama Tempat')
plt.tight_layout()
plt.show()

"""Taman Wisata Alam Punti Kayu dan Kampung Cai Ranca Upas memiliki rata-rata rating tertinggi, menandakan kualitas pengalaman terbaik, sedangkan Curug Cilengkrang meski populer, memiliki rating terendah sehingga perlu evaluasi layanan.

#### 6. Distribusi Nilai Rating dari Pengguna
"""

print(df_rating['user_rating'].value_counts().sort_index())

# Biasanya digunakan untuk melihat distribusi rating dari semua data
plt.figure(figsize=(6,4))
sns.countplot(x='user_rating', data=df_rating)
plt.title('Distribusi Nilai Rating dari Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""Sebagian besar pengguna memberikan rating 3 dan 4, masing-masing sebanyak 273 dan 301 kali. Rating 5 yang merupakan nilai tertinggi justru lebih sedikit (135 kali), hampir sama sedikitnya dengan rating 2 (140 kali).

# **Data Preparation**

#### Menggabungkan dataset rating dan data tempat wisata menggunakan place_id.
"""

# Gabungkan kedua dataset berdasarkan 'place_id'
df = pd.merge(df_rating, df_place, on='place_id', how='inner')
print("Dataset berhasil digabungkan:", df.shape)

df.head()

"""Dataset df_rating dan df_place berhasil digabungkan berdasarkan kolom place_id menggunakan metode inner join. Hasil penggabungan menghasilkan 846 baris dan 15 kolom, yang mencakup data interaksi pengguna dan informasi deskriptif tempat wisata.

#### Removing Duplicates
"""

# Menghapus duplikat berdasarkan kolom 'place_id'
df.drop_duplicates(subset=['place_id'], inplace=True)

"""#### Handling Missing Value"""

df.isnull().sum()

df.dropna(subset=['gallery_photo_img2', 'gallery_photo_img3'], inplace=True)

df.isnull().sum()

# Menangani nilai kosong (drop semua baris yang punya NaN di kolom penting)
important_columns = ['user_id', 'place_id', 'user_rating', 'place_name', 'place_description', 'category']
df.dropna(subset=important_columns, inplace=True)

"""#### Membersihkan format penulisan"""

# Isi nilai kosong pada kolom 'price' dengan 0 (anggap sebagai gratis)
df['price'] = df['price'].replace('-', '0')  # Jika ada tanda '-' sebagai pengganti kosong
df['price'] = df['price'].fillna('0')

# Konversi price ke numerik (hilangkan simbol dan ubah jadi float)
df['price'] = df['price'].replace('[^0-9]', '', regex=True)
df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(0)

"""#### Menghapus kolom yang tidak relevan"""

# Kolom yang dihapus karena tidak relevan untuk model rekomendasi
columns_to_drop = [
    'place_img',
    'gallery_photo_img1',
    'gallery_photo_img2',
    'gallery_photo_img3',
    'place_map',
    'description_location',
]

# Drop kolom dari DataFrame
df.drop(columns=columns_to_drop, inplace=True)

df.head()

"""#### Menampilkan dataset yang sudah dibersihkan"""

cleaned_data = df
cleaned_data

"""##### Melakukan konversi data series menjadi list"""

# Mengubah beberapa kolom menjadi list
list_place_id = df['place_id'].values.tolist()
list_place_name = df['place_name'].values.tolist()
list_user_id = df['user_id'].values.tolist()
list_user_rating = df['user_rating'].values.tolist()

# Menampilkan jumlah elemen pada masing-masing list
print(f"Jumlah data place_id: {len(list_place_id)}")
print(f"Jumlah data place_name: {len(list_place_name)}")
print(f"Jumlah data user_id: {len(list_user_id)}")
print(f"Jumlah data user_rating: {len(list_user_rating)}")

"""#### Menyusun dictionary berisi informasi konten tiap tempat wisata"""

place_content_dict = {
    'place_id': df['place_id'].astype(str).tolist(),
    'place_name': df['place_name'].astype(str).tolist(),
    'place_description': df['place_description'].astype(str).tolist(),
    'category': df['category'].astype(str).tolist(),
    'city': df['city'].astype(str).tolist()
}

# Mengonversi dictionary menjadi DataFrame baru
place_content_df = pd.DataFrame(place_content_dict)

# Tampilkan baris awal dari DataFrame hasil
print("Data konten tempat wisata berhasil dibuat.")
place_content_df.head()

"""Lima kolom teks yaitu place_id, place_name, place_description, category, dan city dipilih sebagai fitur utama untuk sistem content-based filtering. Kolom-kolom ini menggambarkan ragam destinasi ekowisata dengan berbagai kategori dan lokasi, serta dilengkapi deskripsi yang informatif. Informasi ini memberikan dasar yang kuat untuk mengidentifikasi kemiripan antar tempat dan menghasilkan rekomendasi yang lebih relevan.

# **Model Development Dengan Content Based Filtering**

#### Melakukan Assign DataFrame ke Variabel Baru
"""

content_features_df = place_content_df

"""#### Merancang sistem rekomendasi"""

# Inisialisasi TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# Latih model pada kolom 'category'
vectorizer.fit(content_features_df['category'])

# Simpan hasil ekstraksi fitur
tfidf_features = vectorizer.get_feature_names_out()

# Tampilkan total fitur
print(f"Total fitur yang diekstraksi: {len(tfidf_features)}")

# Tampilkan semua fitur
print("\nFitur hasil ekstraksi dari TF-IDF:")
for i, feature in enumerate(tfidf_features, start=1):
    print(f"{i}. {feature}")

"""#### Membangun representasi matriks berdasarkan bobot TF-IDF dari data kategori"""

# Mengonversi konten kategori destinasi wisata menjadi vektor numerik berbasis TF-IDF
tfidf_matrix = vectorizer.transform(content_features_df['category'])

# Menampilkan ukuran dari matriks TF-IDF yang terbentuk
print(f"Dimensi hasil representasi TF-IDF: {tfidf_matrix.shape}")

tfidf_matrix.todense()

# Mengonversi matriks TF-IDF dari bentuk sparse ke dense di dalam DataFrame
df_vector_tfidf = pd.DataFrame(
    tfidf_matrix.todense(),
    columns=vectorizer.get_feature_names_out(),
    index=content_features_df['category']
)

# Menampilkan sebagian data secara acak agar tidak terlalu banyak yang ditampilkan
tfidf_sample = df_vector_tfidf.sample(n=10, axis=0).sample(n=9, axis=1)

# Cetak hasil sampel TF-IDF
tfidf_sample

"""#### Perhitungan similarity"""

# Lakukan perhitungan cosine similarity antar vektor TF-IDF destinasi
similarity_matrix = cosine_similarity(tfidf_matrix)

# Konversi hasil similarity ke dalam format DataFrame
similarity_df = pd.DataFrame(
    similarity_matrix,
    index=content_features_df['place_name'],
    columns=content_features_df['place_name']
)

# Cetak sebagian hasil untuk melihat tingkat kesamaan antar destinasi
print("Contoh cuplikan dari matriks kesamaan antar destinasi ekowisata:")
preview_similarity = similarity_df.sample(n=5, axis=0).sample(n=5, axis=1)
preview_similarity

"""#### Merancang fungsi model rekomendasi"""

class DestinationRecommender:
    def __init__(self, similarity_matrix, content_features_df):
        self.similarity_df = similarity_matrix
        self.place_content_df = content_features_df

    def recommend_by_category_city(self, selected_category, selected_city=None, top_n=5, same_city_only=False):
        filtered_df = self.place_content_df[
            self.place_content_df['category'].str.contains(selected_category, case=False, na=False)
        ]

        if selected_city:
            if same_city_only:
                filtered_df = filtered_df[filtered_df['city'].str.lower() == selected_city.lower()]

        if filtered_df.empty:
            raise ValueError("Tidak ditemukan destinasi yang sesuai dengan kategori dan/atau kota yang dimasukkan.")

        similarity_scores = {}
        for place in filtered_df['place_name']:
            if place in self.similarity_df.columns:
                similar_places = self.similarity_df[place]
                score = similar_places[filtered_df['place_name']].sum()
                similarity_scores[place] = score

        sorted_places = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)
        top_places = [place for place, score in sorted_places[:top_n]]

        rekomendasi_df = filtered_df[filtered_df['place_name'].isin(top_places)][['place_name', 'category', 'city']]
        rekomendasi_df = rekomendasi_df.drop_duplicates(subset='place_name').reset_index(drop=True)

        return rekomendasi_df


# Inisialisasi objek recommender
recommender = DestinationRecommender(similarity_df, place_content_df)

"""#### Penggunaan dari sistem rekomendasi"""

while True:
    kategori = input("Masukkan kategori wisata (contoh: budaya, cagar alam): ")
    kota = input("Masukkan nama kota (opsional, tekan Enter jika ingin semua kota): ")
    same_city_pref = input("Apakah Anda hanya ingin rekomendasi dari kota yang sama? (y/n): ").strip().lower()
    same_city_only = True if same_city_pref == 'y' else False

    try:
        rekomendasi = recommender.recommend_by_category_city(
            selected_category=kategori,
            selected_city=kota if kota else None,
            top_n=5,
            same_city_only=same_city_only
        )

        if not rekomendasi.empty:
            print("\nðŸŽ¯ Rekomendasi Destinasi Wisata:\n")
            print(tabulate(
                rekomendasi.reset_index(drop=True),
                headers="keys",
                tablefmt="grid",
                showindex=True
            ))
        else:
            print("\nTidak ada destinasi yang direkomendasikan.")

    except ValueError as e:
        print(f"\n[!] {e}")

    ulang = input("\nIngin mencari rekomendasi lagi? (y/n): ").strip().lower()
    if ulang != 'y':
        break

"""# **Model Model Development dengan Collaborative Filtering**

#### Menyimpan dataset rating ke variabel rating_df
"""

ratings_df = df_rating
ratings_df

"""#### Melakukan encoding pada user_id dan place_id"""

# Salin data rating ke variabel baru untuk proses encoding
encoded_ratings = ratings_df.copy()

# Buat objek encoder
user_id_encoder = LabelEncoder()
place_id_encoder = LabelEncoder()

# Lakukan transformasi label ke user_id dan place_id
encoded_ratings['encoded_user'] = user_id_encoder.fit_transform(encoded_ratings['user_id'])
encoded_ratings['encoded_place'] = place_id_encoder.fit_transform(encoded_ratings['place_id'])

# Tampilkan beberapa data hasil encoding
print("Contoh hasil encoding user_id dan place_id:")
display(encoded_ratings[['user_id', 'encoded_user']].drop_duplicates().head())
display(encoded_ratings[['place_id', 'encoded_place']].drop_duplicates().head())

"""#### Mengecek Jumlah Entitas & Rentang Rating"""

# Hitung entitas unik pada data pengguna dan destinasi
total_pengguna_unik = ratings_df['user_id'].nunique()
total_destinasi_unik = ratings_df['place_id'].nunique()

# Cek nilai rating minimum dan maksimum dalam dataset
batas_bawah_rating = ratings_df['user_rating'].min()
batas_atas_rating = ratings_df['user_rating'].max()

# Tampilkan hasil statistik secara terformat
print("Informasi Ringkas Dataset 'ratings.csv'")
print("=" * 45)
print(f"Jumlah unik pengguna        : {total_pengguna_unik}")
print(f"Jumlah unik destinasi       : {total_destinasi_unik}")
print(f"Nilai rating paling rendah  : {batas_bawah_rating}")
print(f"Nilai rating paling tinggi  : {batas_atas_rating}")

"""####  Gabung Fitur, Normalisasi Rating, Split Data"""

# Menyusun array fitur dari hasil encoding user dan destinasi
# Menggunakan encoded_ratings yang sudah memiliki kolom 'encoded_user' dan 'encoded_place'
x_values = encoded_ratings[['encoded_user', 'encoded_place']].to_numpy()

# Menormalisasi skor rating ke dalam skala 0â€“1
min_score = encoded_ratings['user_rating'].min() # Menggunakan encoded_ratings
max_score = encoded_ratings['user_rating'].max() # Menggunakan encoded_ratings

y_values = (encoded_ratings['user_rating'] - min_score) / (max_score - min_score) # Menggunakan encoded_ratings
y_values = y_values.to_numpy()

# Tentukan indeks batas untuk split 80%
split_index = int(0.8 * len(encoded_ratings)) # Menggunakan encoded_ratings

# Membagi data ke dalam training dan validation set
x_train = x_values[:split_index]
x_valid = x_values[split_index:]

y_train = y_values[:split_index]
y_valid = y_values[split_index:]

# Menampilkan beberapa baris sebagai sampel
print("Contoh data fitur (x_train):")
print(x_train[:5])

print("\nContoh data label terstandarisasi (y_train):")
print(y_train[:5])

"""#### Merancang Fungsi Rekomendasi"""

class MatrixFactorizationRecommender:
    def __init__(self, num_users, num_places, embedding_size=50, learning_rate=0.001):
        self.num_users = num_users
        self.num_places = num_places
        self.embedding_size = embedding_size
        self.learning_rate = learning_rate
        self.model = self._build_model()

    def _build_model(self):
        # Input layer
        input_pair = tf.keras.Input(shape=(2,))

        # User embedding
        user_input = layers.Lambda(lambda x: x[:, 0])(input_pair)
        user_embedding = layers.Embedding(
            input_dim=self.num_users,
            output_dim=self.embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )(user_input)

        # Place embedding
        place_input = layers.Lambda(lambda x: x[:, 1])(input_pair)
        place_embedding = layers.Embedding(
            input_dim=self.num_places,
            output_dim=self.embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )(place_input)

        # Dot product
        similarity_score = layers.Dot(axes=1)([user_embedding, place_embedding])

        # Final model
        model = tf.keras.Model(inputs=input_pair, outputs=similarity_score)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss='mse',
            metrics=['mae']
        )
        return model

    def train(self, x_train, y_train, x_val, y_val, epochs=20, batch_size=64):
        return self.model.fit(
            x=x_train,
            y=y_train,
            validation_data=(x_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            verbose=1
        )

    def predict(self, x_input):
        return self.model.predict(x_input)

"""#### Inisialisasi dan Training"""

# Ambil jumlah user dan tempat dari dataset
user_total = encoded_ratings['encoded_user'].nunique()
place_total = encoded_ratings['encoded_place'].nunique()

# Buat objek rekomendasi
rekomendasi_model = MatrixFactorizationRecommender(
    num_users=user_total,
    num_places=place_total,
    embedding_size=50
)

# Latih model menggunakan data training dan validasi
training_log = rekomendasi_model.train(
    x_train, y_train,
    x_valid, y_valid,
    epochs=20,
    batch_size=64
)

"""#### Visualisasi Grafik Proses Training"""

def tampilkan_grafik_training(log_histori):
    epoch_range = range(1, len(log_histori.history['loss']) + 1)

    fig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)

    # ===== Plot Mean Squared Error =====
    ax[0].plot(epoch_range, log_histori.history['loss'], color='royalblue', linewidth=2, label='Training MSE')
    ax[0].plot(epoch_range, log_histori.history['val_loss'], color='darkorange', linestyle='--', linewidth=2, label='Validation MSE')
    ax[0].set_ylabel('Mean Squared Error')
    ax[0].set_title('Performa Loss Selama Training')
    ax[0].legend()
    ax[0].grid(alpha=0.3)

    # ===== Plot Mean Absolute Error =====
    ax[1].plot(epoch_range, log_histori.history['mae'], color='green', linewidth=2, label='Training MAE')
    ax[1].plot(epoch_range, log_histori.history['val_mae'], color='red', linestyle='--', linewidth=2, label='Validation MAE')
    ax[1].set_xlabel('Epoch')
    ax[1].set_ylabel('Mean Absolute Error')
    ax[1].set_title('Performa MAE Selama Training')
    ax[1].legend()
    ax[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.show()

# Panggil fungsi untuk menampilkan hasil grafik training
tampilkan_grafik_training(training_log)

"""Insight dari Grafik Proses Training

1. **Training Loss (MSE)**

   Nilai MSE pada data pelatihan menunjukkan penurunan yang konsisten dari awal hingga akhir epoch. Ini menandakan bahwa model mampu belajar dari pola data pelatihan dengan baik.

2. **Validation Loss (MSE)**

   Garis loss validasi terlihat stabil di sekitar angka 0.46, tanpa mengalami penurunan signifikan. Ini menunjukkan bahwa performa model terhadap data validasi tidak membaik, yang bisa menjadi indikasi awal overfitting.

3. **Training MAE**

  Nilai MAE pada data pelatihan terus menurun dari sekitar 0.52 ke 0.26, yang mengindikasikan penurunan rata-rata kesalahan prediksi pada data latih. Artinya, model makin akurat terhadap data yang pernah dilihat.

4. **Validation MAE**

   Garis MAE validasi tetap datar di sekitar 0.51, menunjukkan bahwa model tidak mengalami peningkatan akurasi terhadap data baru (validasi). Hal ini kembali memperkuat dugaan overfitting, karena model belajar terlalu baik pada data latih tapi gagal generalisasi ke data lain.

#### Mendapatkan tempat yang belum direview oleh user tertentu
"""

# Salin data dari cleaned_data dan ratings.csv
place_data = cleaned_data.copy()
rating_data = pd.read_csv('ratings.csv')

# Buat mapping untuk encoded place dan user dari encoded_ratings
encoded_place_map = dict(zip(encoded_ratings['place_id'], encoded_ratings['encoded_place']))
encoded_user_map = dict(zip(encoded_ratings['user_id'], encoded_ratings['encoded_user']))

# Pilih satu user secara acak dari data rating
selected_user = rating_data['user_id'].sample(1).values[0]

# Tempat yang sudah pernah dinilai oleh user ini
rated_places = rating_data[rating_data['user_id'] == selected_user]['place_id'].tolist()

# Dapatkan tempat yang belum pernah dinilai oleh user tersebut
unrated_places = place_data[~place_data['place_id'].isin(rated_places)]['place_id'].tolist()

# Hanya ambil tempat yang ada dalam peta encoding
valid_unrated_places = [pid for pid in unrated_places if pid in encoded_place_map]

# Encode tempat yang belum dinilai
encoded_places = [[encoded_place_map[pid]] for pid in valid_unrated_places]

# Ambil user_id yang telah di-encode
encoded_user_id = encoded_user_map.get(selected_user)

# Buat array gabungan antara user dan tempat untuk prediksi
input_for_prediction = np.hstack((
    np.full((len(encoded_places), 1), encoded_user_id),
    encoded_places
))

"""#### Rekomendasi lokasi wisata terbaik"""

import random

# 1. Temukan user yang memiliki tepat 5 rating
user_rating_counts = rating_data['user_id'].value_counts()
users_with_5_ratings = user_rating_counts[user_rating_counts == 5].index.tolist()

# 2. Pilih salah satu user secara acak dari user dengan 5 rating
selected_user = random.choice(users_with_5_ratings)

# 3. Gabungkan data rating dengan data tempat
user_rated_data = pd.merge(
    rating_data,
    place_data,
    on='place_id',
    how='inner'
)

# Rename kolom rating jika perlu (opsional jika tidak duplikat)
user_rated_data.rename(columns={'user_rating_x': 'user_rating'}, inplace=True)

# 4. Filter berdasarkan user yang dipilih
user_rated_data = user_rated_data[user_rated_data['user_id_x'] == selected_user]

# 5. Urutkan berdasarkan rating tertinggi dan ambil top 5
top_5_favorit = user_rated_data.sort_values(by='user_rating', ascending=False).head(5)

# 6. Tampilkan hasil tempat favorit
print(f"5 Tempat Favorit dari User ID: {selected_user}")
print("=" * 60)
for idx, row in top_5_favorit.iterrows():
    print(f"{row['place_name']}")

# 7. Prediksi rating untuk tempat yang belum direview oleh user
rated_places = user_rated_data['place_id'].tolist()
unrated_places = place_data[~place_data['place_id'].isin(rated_places)]['place_id'].tolist()

valid_unrated_places = [pid for pid in unrated_places if pid in encoded_place_map]
encoded_places = [[encoded_place_map[pid]] for pid in valid_unrated_places]
encoded_user_id = encoded_user_map.get(selected_user)

input_for_prediction = np.hstack((
    np.full((len(encoded_places), 1), encoded_user_id),
    encoded_places
))

predicted_scores = rekomendasi_model.predict(input_for_prediction).flatten()

rekomendasi_df = pd.DataFrame({
    'place_id': valid_unrated_places,
    'predicted_score': predicted_scores
})

rekomendasi_df = rekomendasi_df.merge(place_data, on='place_id')
top_10_rekomendasi = rekomendasi_df.sort_values(by='predicted_score', ascending=False).head(10)

print("\n10 Rekomendasi Tempat Wisata untuk User ID:", selected_user)
print("=" * 60)
for idx, row in top_10_rekomendasi.iterrows():
    print(f"{row['place_name']}")